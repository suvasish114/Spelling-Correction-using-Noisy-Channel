{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1SajsefqzXnZES_ifsRlcjLq76uDSbXTr",
      "authorship_tag": "ABX9TyOmHbcht5nZ9+7aGaz4qFXc"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Spell Checker"
      ],
      "metadata": {
        "id": "YLeuVrpeDv9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string                       # python inbuild string class\n",
        "import numpy as np                  # to handle matrix operations\n",
        "import pandas as pd                 # to handle dataframes\n",
        "import matplotlib.pylab as plt      # plotting diagrams\n",
        "import nltk                         # NLTK package\n",
        "from nltk.util import ngrams        # to get ngram from a given string\n",
        "from collections import Counter     # to get frequency of word from a corpas\n",
        "nltk.download(\"punkt\")              # Essential package for NLTK"
      ],
      "metadata": {
        "id": "Mg8iyav4Yqle",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5ad1690-77d1-4082-dc1b-de2fc707e70b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find Candidate Words"
      ],
      "metadata": {
        "id": "STjRvwerhmm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Used: The YAWL (Yet Another Word List) word list (263,533 words) by Peter Norvig\n",
        "\n",
        "URL: https://norvig.com/ngrams/word.list"
      ],
      "metadata": {
        "id": "WOxjYtOkeMOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DICTIONARY\n",
        "PATH = \"drive/MyDrive/Datasets/word_list.txt\"\n",
        "with open(PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    dictionary = f.read().split(\"\\n\")\n",
        "\n",
        "print(f\"total number of lines: {len(dictionary)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Sp-mBYtrmQq",
        "outputId": "68614489-0e63-4a88-a738-e32593835495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total number of lines: 263534\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ALPHABETS\n",
        "alphabets = list(\"abcdefghijklmnopqrstuvwxyz\")"
      ],
      "metadata": {
        "id": "I9Kb_ZRn_b5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIND ALL CANDIDATES\n",
        "def find_candidate_words(word):                     # get candidate words with single transformation\n",
        "    ''' Find all candidate words for given misspelled word by using {insetion, deletion, transpose, substitution}\n",
        "    Args:   word (string) = misspelled word\n",
        "    Return: (list) a list containing all candidate words '''\n",
        "    candidate_words = []\n",
        "    for char in alphabets:\n",
        "        for idx in range(len(word)+1):              # Insert candidate\n",
        "            candidate_words.append(word[:idx]+char+word[idx:])\n",
        "        for idx in range(len(word)):                # Substitution candidate\n",
        "            candidate_words.append(word[:idx]+char+word[idx+1:])\n",
        "    for idx in range(len(word)):                    # Deletion candidate\n",
        "        candidate_words.append(word[:idx]+word[idx+1:])\n",
        "    if(len(word)>1):                                # Transpose candidate\n",
        "        for idx in range(len(word)-1):\n",
        "            candidate_words.append(word[:idx]+word[idx+1]+word[idx]+word[idx+2:])\n",
        "    return candidate_words"
      ],
      "metadata": {
        "id": "MxuL0tqZ9eH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIND VALID CANDIDATES\n",
        "def find_valid_candidates(word):              # find all valid candidates from dictionary\n",
        "    ''' Find all valid candidate words from a given word which also available in dictionary\n",
        "    Args:   word = a string denoting misspelled word\n",
        "    Return: A list of all valid candidate words '''\n",
        "    candidate_first_edit = find_candidate_words(word)\n",
        "    candidate_second_edit = []\n",
        "    for _candidate_first_edit in candidate_first_edit:\n",
        "        candidate_second_edit += find_candidate_words(_candidate_first_edit)\n",
        "    candidates = set(candidate_first_edit).union(set(candidate_second_edit))\n",
        "    return list(set(candidates).intersection(set(dictionary)))"
      ],
      "metadata": {
        "id": "Y1tDhylpABEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_valid_candidates(word):\n",
        "    all_candidate_words = find_candidate_words(word)\n",
        "    valid_candidates = list(set(all_candidate_words).intersection(set(dictionary)))\n",
        "    return valid_candidates"
      ],
      "metadata": {
        "id": "iKEaakTidge5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_valid_candidates(\"brupt\")"
      ],
      "metadata": {
        "id": "Mt94NNhroFaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62e3a85f-9672-4500-af98-86a2ee1284e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['brunt', 'abrupt', 'erupt', 'bruit', 'brut', 'brust']"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Minimum Edit distance"
      ],
      "metadata": {
        "id": "omhszDeKB0FQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_minimum_transformation(l):\n",
        "    if l[0] <= l[1] and l[0] <= l[2]:\n",
        "        return l[0],'d'\n",
        "    elif l[1] <= l[2] and l[1] <= l[0]:\n",
        "        return l[1],'i'\n",
        "    else:\n",
        "        return l[2],'s'"
      ],
      "metadata": {
        "id": "aZmGwWsLbUHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_minimum_edit_distance(s1, s2):\n",
        "    ''' Calculate the minimum edit distance between two strings\n",
        "        Args:   s1 (string) = first string; error word\n",
        "                s2 (string) = second string; candidate word\n",
        "        Return: (int) minimum edit distance to transform from s1 to s2,\n",
        "                (2D float array) calculate cost matrix '''\n",
        "\n",
        "    m = len(s1)                         # length of source string\n",
        "    n = len(s2)                         # length of target string\n",
        "    mat = np.zeros((m+1, n+1))          # 2D array of dimention (n+1 X m+1) filled with 0's\n",
        "    seq = [[\"\" for a in range(n+1)] for _ in range(m+1)]\n",
        "\n",
        "    # Initialization\n",
        "    for i in range(1, m+1):             # Initializing row 1\n",
        "        mat[i, 0] = mat[i-1, 0] + 1\n",
        "        seq[i][0] = 'd'\n",
        "    for j in range(1, n+1):             # Initializing column 1\n",
        "        mat[0, j] = mat[0, j-1] + 1\n",
        "        seq[0][j] = 'i'\n",
        "    seq[0][0] = 'd'\n",
        "\n",
        "    # Recursion\n",
        "    for i in range(1, m+1):\n",
        "        for j in range(1, n+1):\n",
        "            sc = 2 if s1[i-1] != s2[j-1] else 0\n",
        "            cost, transformation = get_minimum_transformation([mat[i-1, j] + 1,         # deletion\n",
        "                                                               mat[i, j-1] + 1,         # insertion\n",
        "                                                               mat[i-1, j-1] + sc])     # substitution\n",
        "            mat[i, j] = cost\n",
        "            seq[i][j] += transformation\n",
        "\n",
        "    # get transformation sequence\n",
        "    s1 = '#'+s1\n",
        "    s2 = '#'+s2\n",
        "    i,j = len(s1)-1, len(s2)-1\n",
        "    sequence = list()\n",
        "    while True:\n",
        "        if seq[i][j] in ['d','i']:\n",
        "            sequence.append(s1[i]+s2[j]+seq[i][j])\n",
        "            if seq[i][j] == 'd':\n",
        "                i -= 1\n",
        "            elif seq[i][j] == 'i':\n",
        "                j -= 1\n",
        "        else:\n",
        "            if s1[i] != s2[j]:\n",
        "                sequence.append(s1[i]+s2[j]+seq[i][j])\n",
        "            else:\n",
        "                sequence.append('')\n",
        "            i -= 1\n",
        "            j -= 1\n",
        "        if i == 0 and j == 0:   # stopping condition\n",
        "            break\n",
        "\n",
        "    # print(mat)\n",
        "    # print(np.array(seq))\n",
        "\n",
        "    return int(mat[m,n]), sequence"
      ],
      "metadata": {
        "id": "WPKSyv-CB6bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "get_minimum_edit_distance(\"intention\", \"execution\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUkr8x77Y3o-",
        "outputId": "5f681645-e6af-44d0-eb48-3f07987ec7b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8,\n",
              " ['', '', '', '', 'nud', 'eui', 'eci', 'eei', 'exi', '', 't#d', 'n#d', 'i#d'])"
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_minimum_edit_distance('bleu', 'blue')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLxJ1Iv0DXWq",
        "outputId": "fc8a23c6-2cd6-4781-945f-7360d7bc4dee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, ['ued', '', 'lui', '', ''])"
            ]
          },
          "metadata": {},
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find Prior and Channel probability\n",
        "\n"
      ],
      "metadata": {
        "id": "NjRaI7hkl-Yn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prior Model"
      ],
      "metadata": {
        "id": "vzRWzU2NmOB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove punchuation\n",
        "import string\n",
        "strip_chars = string.punctuation\n",
        "def remove_punc(sentence):\n",
        "    return sentence.translate(str.maketrans('', '', string.punctuation)).lower()"
      ],
      "metadata": {
        "id": "izrWpl1neqB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_punc(\"Personal Story: Mayor Jerry Brown.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FTFYPeKqexpK",
        "outputId": "f73fd8f0-d9cc-4cb1-ea09-284856e689d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'personal story mayor jerry brown'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GET TRAINING SET FOR TOKENS\n",
        "PATH = \"drive/MyDrive/Datasets/coca_sources.txt\"\n",
        "with open(PATH, 'r', encoding='utf8', errors='ignore') as f:\n",
        "    sentences = [remove_punc(a.split(\"\\t\")[-1]).lower() for a in f.read().split(\"\\n\")[2:]]\n",
        "\n",
        "# Test\n",
        "sentences[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAukNvJPmFPz",
        "outputId": "413d794c-f736-4b9a-e979-447d9e00c54e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['interview with byron york',\n",
              " 'political headlines',\n",
              " 'special report roundtable',\n",
              " 'impact is the economy heading into a recession',\n",
              " 'personal story mayor jerry brown']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_BWfKfee_gx",
        "outputId": "ab4b6423-2807-4035-e316-386ecbeb75c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "189746"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MAKE N-GRAM\n",
        "def extract_ngrams(sentence, n):\n",
        "    n_grams = ngrams(nltk.word_tokenize(sentence), n)\n",
        "    # return [ ' '.join(grams) for grams in n_grams]\n",
        "    return [grams for grams in n_grams]"
      ],
      "metadata": {
        "id": "Jb3PVw70fb4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_ngrams(\"impact is the economy heading into a recession\",3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YOPCctDgou3",
        "outputId": "e36116a6-b214-48fc-e08a-879270905d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('impact', 'is', 'the'),\n",
              " ('is', 'the', 'economy'),\n",
              " ('the', 'economy', 'heading'),\n",
              " ('economy', 'heading', 'into'),\n",
              " ('heading', 'into', 'a'),\n",
              " ('into', 'a', 'recession')]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPUTE ALL NGRAM FOR ENTIRE CORPUS\n",
        "def get_all_ngrams(sentences, n):\n",
        "    ngram_list = []\n",
        "    for sentence in sentences:\n",
        "        ngram_list += extract_ngrams(sentence, n)\n",
        "    return ngram_list"
      ],
      "metadata": {
        "id": "leXqDLUchIo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AGGREGRATE NGRAMS\n",
        "def calculateCounts(sentences, n):\n",
        "    ngramList = get_all_ngrams(sentences, n)\n",
        "    ngramCount = Counter(ngramList)\n",
        "    return ngramCount"
      ],
      "metadata": {
        "id": "RKChzM3rt5Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_count = calculateCounts(sentences, 1)  # unigram\n",
        "bigram_count = calculateCounts(sentences, 2)   # bigram"
      ],
      "metadata": {
        "id": "sjZFAl7doVQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_count[('there', 'is')]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeF939yDVSIb",
        "outputId": "0996b315-2041-4e46-8b01-dc8bf260143b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GET PRIOR PROBABILITY\n",
        "def get_prior_probability(word):\n",
        "    return unigram_count[tuple(word.split())]/len(unigram_count)"
      ],
      "metadata": {
        "id": "lbpl4wCwlcVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bigram_probability(word):\n",
        "    return bigram_count[tuple(word.split())]/len(bigram_count)"
      ],
      "metadata": {
        "id": "bP2brL8Asknx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST\n",
        "cases = [\"to\",\n",
        "         \"once\",\n",
        "         \"there\",\n",
        "         \"they\"]\n",
        "for a in cases:\n",
        "    print(f\"{a}\\t:{get_prior_probability(a)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZzWaanaodMr",
        "outputId": "a3e053dd-d908-4561-f0ad-d273877e07fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "to\t:0.32552125054356534\n",
            "once\t:0.0029981919300574463\n",
            "there\t:0.006122262147254709\n",
            "they\t:0.01360629849174925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO: Creating Confusion Matrix"
      ],
      "metadata": {
        "id": "fc8LcWK2o97E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# confusion matrix for insertion\n",
        "with open(\"drive/MyDrive/Datasets/confusion_mats/add_mat.txt\", 'r', encoding='utf8', errors='ignore') as f:\n",
        "    lines = f.read().strip().split(\"\\n\")\n",
        "ins_mat = [line.strip().split() for line in lines]"
      ],
      "metadata": {
        "id": "G8AJpDrdEmR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confusion matrix for deletion\n",
        "with open(\"drive/MyDrive/Datasets/confusion_mats/del_mat.txt\", 'r', encoding='utf8', errors='ignore') as f:\n",
        "    lines = f.read().strip().split(\"\\n\")\n",
        "del_mat = [line.strip().split() for line in lines]"
      ],
      "metadata": {
        "id": "jYfA71MTFcJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confusion matrix for subtraction\n",
        "with open(\"drive/MyDrive/Datasets/confusion_mats/sub_mat.txt\", 'r', encoding='utf8', errors='ignore') as f:\n",
        "    lines = f.read().strip().split(\"\\n\")\n",
        "sub_mat = [line.strip().split() for line in lines]"
      ],
      "metadata": {
        "id": "vhcodSpbFcSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confusion matrix for transpose\n",
        "with open(\"drive/MyDrive/Datasets/confusion_mats/rev_mat.txt\", 'r', encoding='utf8', errors='ignore') as f:\n",
        "    lines = f.read().strip().split(\"\\n\")\n",
        "trans_mat = [line.strip().split() for line in lines]"
      ],
      "metadata": {
        "id": "F_L5nyRHFcU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Channel Model"
      ],
      "metadata": {
        "id": "R4Mq4YJQGNCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# COUNT MATRIX\n",
        "def get_count(lexem):\n",
        "    unigram_count = dict()\n",
        "    bigram_count = dict()\n",
        "    for sentence in sentences:\n",
        "        bigrams = [\"\".join(a) for a in list(ngrams(lexem, n=2))]\n",
        "        unigrams = [\"\".join(a) for a in list(ngrams(lexem, n=1))]\n",
        "        bcount = Counter(bigrams)\n",
        "        ucount = Counter(unigrams)\n",
        "\n",
        "        for key in bcount:  # Count all bigrams\n",
        "            try:\n",
        "                bigram_count[key] += bcount[key]\n",
        "            except:\n",
        "                bigram_count[key] = bcount[key]\n",
        "\n",
        "        for key in ucount:  # count all unigrams\n",
        "            try:\n",
        "                unigram_count[key] += ucount[key]\n",
        "            except:\n",
        "                unigram_count[key] = ucount[key]\n",
        "\n",
        "    return unigram_count, bigram_count"
      ],
      "metadata": {
        "id": "Igu_sM7zdcwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST\n",
        "a,b = get_count(\"have\")\n",
        "print(a)    # unigram's frequency\n",
        "print(b)    # bigram's frequency"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-qaUhtcoTBR",
        "outputId": "7e5c80f7-2ef2-49cc-d8f4-8ee689dbd3ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'h': 189746, 'a': 189746, 'v': 189746, 'e': 189746}\n",
            "{'ha': 189746, 'av': 189746, 've': 189746}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PROBAIBLITY OF CHANNEL\n",
        "def char_probability(x, y, type):\n",
        "    xi = (ord(x)-ord('a'))    # normalization\n",
        "    yi = (ord(y)-ord('a'))\n",
        "    uni_count, bi_count = get_count(x+y)\n",
        "    if type.lower() == \"d\":\n",
        "        return int(del_mat[xi][yi])/int(bi_count[x+y])\n",
        "    elif type.lower() == \"i\":\n",
        "        return int(ins_mat[xi][yi])/int(uni_count[y])\n",
        "    elif type.lower() == \"s\":\n",
        "        return int(sub_mat[xi][yi])/int(uni_count[y])\n",
        "    elif type.lower() == \"t\":\n",
        "        return int(trans_mat[xi][yi])/int(bi_count[x+y])\n",
        "    return -1"
      ],
      "metadata": {
        "id": "j0PIk3Oz8VbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_probability('h', 'a', 'd')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ED3MwIDlvGDP",
        "outputId": "5136bf42-93ea-4765-a0b8-622127cbf3a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.905304986666386e-05"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_channel_probability(word, candidate):\n",
        "    edit_distance, edit_sequence = get_minimum_edit_distance(word, candidate)\n",
        "    probability = 0\n",
        "    for sequence in edit_sequence:\n",
        "        if sequence != '':\n",
        "            sequence_list = list(sequence)\n",
        "            probability += char_probability(sequence_list[0], sequence_list[1], sequence_list[2])\n",
        "    return probability"
      ],
      "metadata": {
        "id": "vZRZizTvzHyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Estimation"
      ],
      "metadata": {
        "id": "9Ul0jNVzpsXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Estimation\n",
        "def get_correct_word(misspelled_word, prev_word, next_word):\n",
        "    probable_candidate_words = {}\n",
        "    probable_bi_gram = {}\n",
        "    candidates = find_valid_candidates(misspelled_word.lower())\n",
        "    for candidate in candidates:\n",
        "        prior_probability = get_prior_probability(candidate)\n",
        "        channel_probability = get_channel_probability(misspelled_word, candidate)\n",
        "        prob = prior_probability * channel_probability\n",
        "        probable_candidate_words[candidate] = prob\n",
        "    probable_candidate_words = sorted(probable_candidate_words.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    for candidate in probable_candidate_words:\n",
        "        probable_bi_gram[prev_word+\" \"+candidate[0]+\" \"+next_word] = get_bigram_probability(prev_word+\" \"+candidate[0]) * get_bigram_probability(candidate[0]+\" \"+next_word)\n",
        "    probable_bi_gram = sorted(probable_bi_gram.items(), key=lambda x: x[1], reverse=True)\n",
        "    return probable_candidate_words, probable_bi_gram\n"
      ],
      "metadata": {
        "id": "XPWzL7djpAXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test suit\n",
        "import pprint\n",
        "test_cases = [\"a goxd mine\",\n",
        "              \"wear blxe shirt\",\n",
        "              \"the nbw era\",\n",
        "              \"skyroot rockt company\",      # prediction error\n",
        "              \"the vintge computer\"]\n",
        "for cases in test_cases:\n",
        "    print(f'Given: {cases}')\n",
        "    words = cases.split()\n",
        "    a, b = get_correct_word(words[1], words[0], words[2])\n",
        "    print(f'Correct: {b[0][0]}')\n",
        "    print(\"\\nStatus:\")\n",
        "    pprint.pprint(a)\n",
        "    pprint.pprint(b)\n",
        "    print(\"==\"*30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-T-cmGBei82l",
        "outputId": "4db82afb-aad5-4482-c221-82cc01098226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given: a goxd mine\n",
            "Correct: a gold mine\n",
            "\n",
            "Status:\n",
            "[('good', 2.522383263240755e-06),\n",
            " ('gold', 1.0095805237818057e-07),\n",
            " ('gowd', 0.0),\n",
            " ('gox', 0.0),\n",
            " ('god', 0.0),\n",
            " ('goad', 0.0)]\n",
            "[('a gold mine', 4.9295163515489077e-11),\n",
            " ('a good mine', 0.0),\n",
            " ('a gowd mine', 0.0),\n",
            " ('a gox mine', 0.0),\n",
            " ('a god mine', 0.0),\n",
            " ('a goad mine', 0.0)]\n",
            "============================================================\n",
            "Given: wear blxe shirt\n",
            "Correct: wear blue shirt\n",
            "\n",
            "Status:\n",
            "[('blue', 5.156459664476964e-08), ('blae', 0.0), ('blee', 0.0)]\n",
            "[('wear blue shirt', 0.0), ('wear blae shirt', 0.0), ('wear blee shirt', 0.0)]\n",
            "============================================================\n",
            "Given: the nbw era\n",
            "Correct: the new era\n",
            "\n",
            "Status:\n",
            "[('new', 3.563179968570319e-05),\n",
            " ('now', 1.0697693746022501e-06),\n",
            " ('naw', 1.0252609859193965e-09)]\n",
            "[('the new era', 9.751815722451626e-08),\n",
            " ('the now era', 0.0),\n",
            " ('the naw era', 0.0)]\n",
            "============================================================\n",
            "Given: skyroot rockt company\n",
            "Correct: skyroot rocks company\n",
            "\n",
            "Status:\n",
            "[('rocks', 4.824757580797159e-07),\n",
            " ('rocky', 2.2435122750706787e-08),\n",
            " ('rocket', 1.8454697746549136e-08),\n",
            " ('rock', 0.0)]\n",
            "[('skyroot rocks company', 0.0),\n",
            " ('skyroot rocky company', 0.0),\n",
            " ('skyroot rocket company', 0.0),\n",
            " ('skyroot rock company', 0.0)]\n",
            "============================================================\n",
            "Given: the vintge computer\n",
            "Correct: the vintage computer\n",
            "\n",
            "Status:\n",
            "[('vintage', 1.1525139671129213e-07)]\n",
            "[('the vintage computer', 0.0)]\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}